================================================================================
      _            _              _____ _          _ _ 
     | |          | |            / ____| |        | | |
   __| | ___ _ __ | | ___  _   _| (___ | |__   ___| | |
  / _` |/ _ \ '_ \| |/ _ \| | | |\___ \| '_ \ / _ \ | |
 | (_| |  __/ |_) | | (_) | |_| |____) | | | |  __/ | |
  \__,_|\___| .__/|_|\___/ \__, |_____/|_| |_|\___|_|_|
            | |             __/ |                      
            |_|            |___/                       
================================================================================

The MIT License (MIT)

Copyright (c) 2014 Sam Caldwell.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

================================================================================
Table of Contents
================================================================================


I.      Introduction

I. A. 	What is deployShell?
I. B.	Why Adopt deployShell?


II.     Installing and Using deployShell

II. A.	Installation
II. B.	Getting Started
II. C.	Adding and Deleting Cloud-Provider Accounts
II. D.	Adding and Deleting Users
II. E.	Access Control Lists (User Permissions within deployShell)
II. F.	Creating, Listing and Deleting Server Instances.


III.    Architecture: An Overview

III. A.	The Shell User Interface.
III. B. Command Structure and Processing
III. C. FHS Compliance
III. D.	Mapping Operations
III. E. Evaluating Recipes Against Maps 
IV. 	Command Reference.
V.		Mapping: Abstracting Cloud Resources to Integrate Cloud Providers
VI.     Recipes: A Beginning
VII.    Recipes: Adding Tags, Pre-Deploy and Post-Deploy Scripts
VIII.   Recipes: Quality Assurance through Unit Tests
IX.     Recipes: Advanced Recipes for Building Clusters
X.      User Management in deployShell
	
================================================================================
I. 	Introduction
================================================================================



I. A.	What is DeployShell?

The deployShell is an interactive user shell providing users with a means of 
deploying cloud infrastructure (e.g. servers) across a wide variety of cloud 
hosting providers through a recipe-based approach that is vendor agnostic.  The
benefit is a centralized interface designed to interact with multiple cloud 
providers through a single user-management and access control system, reducing 
cost of operations and freeing companies from their Operations/IT departments.

The business case for deployShell is more than building a more flexible 
organization with an ability to rapidly deploy and iterate through full-stack 
software builds.  deployShell also allows organizations to locate servers 
strategically among the cloud providers in near-realtime for cost effectiveness
as well as vendor and geographic diversity.

This project embraces the following design principles:

    (1) Start with a common denominator and add no new deployment dependencies.

    (2) Ensure adequate security around the deployment mechanism both for
        external threats and internal controls.

    (3) Build an out-of-the-box, simple-to-learn/simple-to-use solution that
        will lower labor costs for consuming organizations and reduce ramp-up
        time.

    (4) Keep it scalable.  The solution should be able to add new cloud
        providers with little pain or re-write.



I. B.	Why Adopt deployShell?

The first question one has to confront when introducing a new technology is 
"Why not use [enter existing technology here]?"  In the case of deployment
automation, that question might be "Why not use Chef?"  "Why not use Puppet?"
"Why not use PXE and Kickstart?"  The answer two part: simplicity and the need
for a comprehensive solution.  PXE will install an operating system. Kickstart
will configure an installed environment with custom parameters.  Chef, Puppet
and Capistrano will orchestrate cloud environments to build vendor-specific
custom environments.  But none of these solutions bring all cloud vendors into
one cohesive ecosystem organizations can leverage to deliver their solution
with minimum effort.

deployShell is a complete framework which can be adapted to private and public
cloud deployment needs.  The vision not only allows for public cloud vendor
support but an extensibility which may come to cover the private cloud products
as well as hypervisor deployments.  In the end, the deployShell offers the 
greatest potential for full-service deployment automation.



I. B.	Use-Cases for deployShell 

I. B. 1.	Deploying "Fresh" Development/QA Environments

The famous joke in software development occurs from time to time when the
software engineer shrugs and explains to the support/operations staff (or, god
forbid, the customer) and says dismissively "It works on my machine."  For 
everyone, including the software engineer, this answer is frustrating and 
unacceptable.  Reducing this almost unavoidable eventuality is critical in 
maintaining an organizations viability over the long haul.  deployShell 
provides one strong answer.

The root cause of most developer-to-production discrepancy is bad practice with
respect to development and QA environments.  These environments are almost 
always reused, especially in the startup world where time and skill are a 
premium.  As a result these environments become unstable over time and require
manual interventions to ensure systems work properly when they should simply be
re-deployed from scratch before each use.

In the "old days" (pre-cloud) this was not always possible unless a 
considerable amount of time was spent building Ghost images, Ghost or PXE 
servers, Kickstart files or other means of re-deploying bare metal systems.  
Virtualization improved this situation some, but the "cloud" hosting paradigm 
with per-hour pricing and out-of-the-box machine instantiation has made any 
excuse against re-deployment a moot point.  

Development and QA servers are often only used during business hours.  Assuming
a twelve hour work day, this means that only 50% of the cost of a cloud-based 
development or QA server is actually needed.  Companies are effectively spending
their resources without any real benefit for 24x7 servers and these static, 
often "dirty" development servers are causing real problems over time that cost
even more money and efficiency.

Organizations could instead use deployShell to automate their deployment and 
destruction of development servers such that only those development environments
which are needed are actually "spinning" at any given time.  When developers
need an environment, they can deploy one within the established deployShell 
user-privilege and automation policy framework.  As a result, each environment
is maintained as a "fresh" environment for real work while the organization
saves its cash for real growth.

The objection most developers raise to the perpetual deployment is the latency
between their request for an environment and the Operations' team's delivery of
this environment.  The deployShell, however, eliminates the need for a large 
Operations team in favor of granting the developer him/herself access to deploy
servers within the user's allowed policy.  This policy may determine to which 
cloud account a developer can deploy as well as the maximum lifetime and 
operating hours of the server.  The Operations team shifts under this paradigm 
to an audit-support-enabler role for the organization rather than an expensive 
blocker to the development team.



I. B. 2.	Deploying Flexible Production Environments

The "cloud" promises to be cost effective.  However, most organizations have 
seen that without flexible automation, its only benefit is transferring cost 
from CapEx to OpEx on the financial statements.  For "cloud" to be cost 
effective, the environments have to be vendor agnostic and flexible.  Pricing 
should determine (in part) the vendor to be selected and more than one vendor 
should be used to avoid costly downtime.  Additionally the automation for 
deploying servers to the target environment must not become a leash held by the
cloud hosting provider.

When an organization chooses deployShell, it chooses to focus on the "recipes" 
that create their solutions rather than satisfying and learning the vendor's API
and billing ins and outs.  These recipes define the parameters of the server 
machine as well as budget allowed per instance in addition to the instructions 
needed to install/create the server instance regardless of the vendor selected.

When a server is deployed, the deployShell uses its parameters to determine 
server size, location based on the latest information pulled from each allowed 
cloud-hosting provider's API.  This means that a single recipe can define a 
server to be deployed, regardless of the target hosting provider.  As a network
grows, or as customer base shifts geographic center, the server deployments can
be shifted accordingly.  Further, as hosting costs change, servers can be 
redeployed across the Internet to ensure efficiency as well as reliability and
security.



================================================================================
II.     Installing and Using deployShell
================================================================================



II. A.	Installation

The deployShell runs on top of any Debian 7+ or Ubuntu 14.04+ Server via a 
simple install script.  The systems administrator only needs to execute the 
following line to perform the install process:


	curl -s 'https://github.com/x684867/installDeployShell/setup' | sudo bash


This script will generate an SSH key to be used when connecting as the server
administrator.  This key will be installed on the target server's default user
(deploy@server.tld).



II. B.	Getting Started


II. B. 1.	Configuring the New Server

After installing the deployShell, the systems administrator can log in via
ssh using the following command--


	ssh -i ~/.ssh/deployShellAdmin deploy@server.tld

	
When the user connects as the deploy user under the deployShell, he/she will
have elevated privileges for the system.  The systems administrator should then 
execute the following command to configure the server--

	server.configure --interactive
	
This will provide an interactive setup wizard to assist the administrator in
configuring email, authentication and other system-level aspects of the server.
The wizard will also allow the systems administrator to configure the initial
"user admin" and "hosting admin" accounts.  These users will then be emailed by
the system with their log in credentials.

NOTE: Once the "server.configure" command completes and the administrator user
logs out, deployShell will delete the private SSH key associated with the deploy
user, preventing any person from ever logging in as this user going forward.
This is a security feature intended to enforce user best practices moving on.


II. B. 2.	Basic User-Account Roles

The deployShell has three (3) basic user roles (not including the initial deploy
user account used to configure the system):

	"user admin"
	"hosting admin"
	"deployer"
	
The "user admin" is the group of users who create and manage all user accounts
on the system.  This includes creating user groups and user-focused policies as
well as applying system policies to the user groups.  (NOTE: policies are not
enforced on a user-by-user basis but on a group-basis.)

The "hosting admin" is the group of users who create and manage cloud provider
accounts used to connect to the hosting providers' various API.  This includes
creating and applying use policies to the cloud hosting accounts.  (NOTE: The
hosting policies are applied account-by-account to the cloud hosting providers
by the hosting admin either globally or "perUserGroup."  The hosting admin then
can control the behavior of user groups while the user admin controls to which
groups users belong or which groups exist.

The "deployer" is the group of end users created by the user admin.  These are
the users who are allowed through policy to create infrastructure (e.g. server 
instances) on the various cloud hosting provider accounts. 



II. C.	Adding and Deleting Cloud-Provider Accounts

This section covers "cloud-provider accounts."  A "cloud provider" is a vendor
such as Amazon AWS, Google Compute Engine, Digital Ocean, Rackspace or even an 
internal enterprise virtualization infrastructure.  In this section, deployShell
commands will be covered that help the "hosting admin" users connect their
existing cloud hosting accounts to the deployShell command and control software.



II. C. 1. Creating Cloud Hosting Accounts

Cloud hosting accounts must be created through the respective cloud hosting 
provider's unique mechanism.  Instructions for each provider are given during
the execution of the addCloudProviderAcct command--


	cloudProvider.account.add myAccountName amazonAWS
	
	
Executing the above command will create a deployShell account file with the name
"myAccountName" using the deployShell-AmazonAWS API interface module.  This 
module will provide all AmazonAWS-specific content, instructions and automation.


	Note: By design creating cloud-provider accounts is a manual activity.
	While it was considered to make this a "silent" command which could be
	automated for a rapid deployment of accounts, the use-case did not 
	really exist.  The reader is encouraged to provide feedback on any 
	specific use case where rapid and frequent account creation is 
	considered appropriate.



II. C. 2. Associating Cloud-Hosting Accounts with Policies

Once a cloud-provider account is established (II.C.1) the account exists as a 
file on the deployShell server file system in a "disabled" state.  The account
cannot be used until it is enabled (II.C.3).  This gives the "hosting admin" an
opportunity to first associate the account with any use policy files.

Policy files allow the hosting admin to set limits on who can use a given cloud
provider account and how.  Currently only the who and how many machines limits
are in place but the long term goal is to allow a hosting admin to set budget,
performance, geography and other limitations.  These policy constraints can help
an organization to better manage and account for resource usage.

To associate an account with a policy, the hosting admin would execute--


	cloudProvider.account.policyApply myAccountName myPolicyName enforce

	
When a policy is applied to an account, the policy is effective prospectively.
This is important since it means that any pre-existing servers or infrastructure
will not be removed by the policy.  Only new assets will be prevented from being
created once the policy is in place.  However, a hosting admin can audit the 
account for policy violations to determine what pre-existing environments exist
that would violate the policy:


	cloudProvider.account.policyAudit myPolicyName

	
Because cloud account policies could cause significant problems in a flexible 
and mature cloud infrastructure, there is (as illustrated, above) an "enforce"
argument passed to "policyApply."  This is an optional argument.  By default,
the command assumes "passive" and only applies the policy in a passive mode that
has no actual effect on operations.  Thus a hosting admin could execute--


	cloudProvider.account.policyApply myAccountName myPolicyName passive

	
Then, in a passive policy mode, the hosting admin could use "policyAudit" to 
inspect the likely effects the policy would have before enabling the policy:


	cloudProvider.account.enforcePolicy myPolicyName

	
This can later be disabled by executing--


	cloudProvider.account.relaxPolicy myPolicyName


A policy can be dissociated from a cloud-provider account using--

	
	cloudProvider.account.removePolicy myAccountName myPolicyName
	
	
This would both disable and remove the policy association and any constraints it
implies.



II. C. 3. Enabling/Disabling Cloud-Hosting Accounts

Once a hosting admin is satisfied that the cloud-provider account is properly
configured, the same can be brought online when the hosting admin executes--


	cloudProvider.account.enable myAccountName


The account can later be disabled by executing--


	cloudProvider.account.disable myAccountName
	

Disabling a cloud provider account is prospective only.  It will not remove or
otherwise affect any existing infrastructure.  It will only prevent deployers 
from creating new assets under this account.  The hosting provider can list the
assets on the account and determine the best course of action.  See II.G. for
more instructions on how to create, list and delete server instances.



II. C. 4. Deleting Cloud Hosting Accounts

The hosting admin can also delete an account by executing--

	
	cloudProvider.account.delete myAccountName
	

Deleting a cloud hosting account is only possible if deployShell sees no active
assets on the account.  If there are any server instances or other assets active
on a given cloud-hosting account, it *will not* delete the account.  This is by
design because account deletion dissociates deployShell from the cloud hosting
account completely.  That is, once an account is deleted, no further API
communication is possible.



II. D.	Adding and Deleting Users

II. D. 1. Creating a New User Account

Users in the deployShell environment are Linux users on its server.  This allows
the deployShell to leverage SSH, PAM and other existing authentication and user
management features for deployShell.  The only difference between a deployShell
user and a standard Linux user (from the perspective of the Linux operating
system) is the shell defined in /etc/passwd.  However, it is not advised for a
system administrator to create deployShell users through the Linux user 
management mechanisms.  There are deployShell-specific assets created and linked
during user creation which would not exist if the user account were created by
the Linux useradd or other command.

To create a new user account, either the systems administrator or a user admin
must log into the deployShell server and execute the following command--


	user.create <role> userName --email email@myFQDN.tld  --key"<sshPubKey>"
	
This command would create a new user with the specified role (e.g. deployer,
userAdmin, hostingAdmin and provide both an email address and SSH public key for
authenticating the user into the system.


II. D. 2. Deleting a User Account

To delete a user account, simply execute--

	
	user.delete userName [--purge]
	

Without the --purge argument, this command will disable a user's account but it
will NOT remove the user account completely from the system, preserving the UID
and GID for the future if the user is restored.  If the --purge argument is used
then the user account will be removed completely, which means re-creating the 
user account will most likely result in a new UID/GID.


II. D. 2. LDAP/OAUTH Authentication

Currently (under version 0.01), only SSH-key authentication against a local 
key is supported by the DeployShell.  However, there are long-range plans to
support multifactor authentication via LDAP/OAUTH once the system's core 
features are completed.



II. D. 3. Adding User Groups

The user admin can create groups for managing users on a role-by-role basis by
executing--


	group.create groupName

	
The users can then be added to this group by executing--


	group.addUser groupName myUsername


Batches of users can be added by executing--


	group.addUsers groupName "<comma-delimited list of usernames>"
	

Once the group is added, it will appear to both the user and hosting admins when
they execute--

	
	group.list
	
	
This command will list all groups in the system.  However, the hosting admins
will not have access to which uses are members of which groups (a feature for
added security).  They can only see the names of the groups in order to attach
their hosting policies to these groups.  But user admins can execute the 
following to see the members in a given group:


	group.members groupName


The user admins can also remove users from a group--

	
	group.removeUser myGroupName myUserName
	

And the user admin can delete the group entirely (and all of its memberships
will be removed--


	group.delete myGroupName

	
However, a user group cannot be deleted until the hosting admins have removed 
all policies attached to the same by executing--	


	group.removePolicy myGroupName myPolicyName
	
	
Policy is applied to the group by executing--


	group.addPolicy myGroupName myPolicyName [enforce|passive]


Note that the addPolicy command will allow the user admin to apply policy to a
group in either an "enforce" or "passive" mode.  In an "enforce" mode, the user
is actively constrained by the policy.  However in a passive mode, the user is
not constrained but violations of the policy are logged and may be audited.

Only the user admins can create and delete groups of users and assign their
memberships and only hosting admins can create and apply policy.  But both user
and hosting admins can see the effect of their actions by auditing a group-to-
policy assignment using--

	
	group.auditPolicy myGroupName
	

The above command will return the list of policy violations in the logs that are
observed under the policy.  This not only allows for a non-intrusive observation
of a deployer's deviation from policy expectation.  It allows for a means of 
pre-testing the policy's effect on the user before committing it into action.

[NOTE: All policy is prospective and not retroactive.  Non-enforced policy will
not have a retrograde effect on actions taken prior to the time when the policy
was put into an enforcing state.]



II. E.	Access Control Lists (User Permissions within deployShell)

II. E. 1. Access Control Through Role-based User Groups

The recommended best practice (enforced by deployShell design) is to apply 
security at the role-based group level.  This prevents one-time user-level
permission grants from creating security and functionality problems down the
road.  To that effect, all security is applied first at the "role designation" 
level of account creation when a user is designated as a "user admin," "hosting
admin" or "deployer."  This is then carried forward by allowing user admins to
create groups of "deployer" user accounts which can be managed by policy.

The vision is that the "user admin" is not necessarily a technical person.  For
that matter, even the "hosting admin" does not need to be a very technical 
employee within an organization.  The two roles could be divided among human
resources (for user admin privileges) and finance (for hosting admin privileges)
with the deployers being the engineers creating the technology behind the 
organization.

Further, the separation of duties in deployShell prevents the "user admin" from
also being a "hosting admin" so that there is less likelihood of fraud within or
error within an organization.  For this reason, the "deployer" cannot also be an
admin (for either user or hosting).  While policy may become technical within 
the recipes created by deployers, the polices created and enforced by user and
hosting admins only relate to the financial and legal side of the business.



II. E. 2. Access Control Through Policy Enforcement

User admins might create and enforce specific policies that limit to which 
servers a given user might have access.  Likewise, the user admin may limit to 
which accounts a deployer can deploy environments (separating development from
QA and production environments).  This form of access control is maintained at
the group level to avoid oversight, errors and any discrimination against a
particular person.  One might say that the purpose of deployShell is not only
to automate deployment but to keep the organization's security practices above
board.



II. E. 3. Server Control Through Policy Enforcement

Policy is enforced against cloud accounts by the hosting admins.  This means a
given recipe may request certain resources that are outside of policy.  Rather
than finding out that the server was excessive during the next billing cycle
policy would prevent the recipe from ever finding a hosting solution among the
available accounts.

But server control through policy enforcement also allows the server's lifespan
and operating hours or scalability to be limited.  The recipe could define that
the server has other server dependencies.  If those dependencies are not met or
attainable, the recipe cannot be processed (again mitigating risk and waste).

Additionally, policy definitions can limit the operating hours of a given server
based on its current deployment state.  For example, a development server may 
only run during business hours.  A QA server likewise.  Additionally the 
development server may only be allowed to last through the current 14 day sprint
before it must be re-deployed, ensuring quality of the environment.


II. F.	Creating, Listing and Deleting Server Instances.


=============================================================================
III.	 Architecture: An Overview
=============================================================================



III. A.	The Shell User Interface.

The deployShell is an interactive shell just like bash, csh, zsh or even the
DOS command line.  The interactive shell is intended to allow many users to 
connect via key-based SSH and manage distributed cloud environments.  The shell
is written in Python with curses providing screen management.  User activity 
is logged through logger.py for in-depth audit trails.

Each user account for the deployShell is a standard Linux user.  The user's
shell in /etc/passwd is defined as /opt/deployShell/deployShell.  Each user has
a home directory in /home/<username> with a default .profile file located 
therein.  This Linux-integration makes the system more efficient to deploy and
maintain within the POSIX/FHS standards.


III. B. Command Structure and Processing




III. C. FHS Compliance

By design, the deployShell is intended to comply with the Linux File Hierarchy
Standard (FHS):

	-----------------------------------------------------------------------
	Path                                      Description
	-----------------------------------------------------------------------
	/usr/local/bin/deployShell                deployShell main file
	/usr/local/lib/deployShell/*              supporting deployShell files
	/etc/deployShell/deployShell.conf         main configuration file
	/etc/deployShell.d/*                      supporting configurations
	/etc/deployShell.d/profile.d/             deployShell profile extensions
	/etc/deployShell.d/profile.d/profile	  global profile extension
	/etc/deployShell.d/profile.d/userAdmin    userAdmin profile extension
	/etc/deployShell.d/profile.d/hostingAdmin hostingAdmin profile extension
	/etc/deployShell.d/profile.d/deployers    deployer users profile extension
	/var/log/deployShell.d/*                  log directory
	/var/lib/deployShell/cloudAccounts/*      cloud hosting accounts
	/var/lib/deployShell/policies/*           deployment policies
	-----------------------------------------------------------------------







III. D.	Mapping Operations



III. E. Evaluating Recipes Against Maps 


=============================================================================
IV.	 Command Reference
=============================================================================

This section identifies all commands available in deployShell along with the
syntax and usage notes for the same.  Looking at the structure of deployShell
commands, the reader should note that commands are structured with a dotted-
tree syntax.  The left-most term in each command identifies the namespace of the
command (e.g. server, cloudProvider, user, group, etc) in order to provide a 
context for the command.  Several commands have sub-spaces (used to clarify the
command's context) with a right-most term providing the context-specific
command.  An example might be--


group.addPolicy


This command belongs to namespace "group" with a context-specific command of
"addPolicy."  Likewise--


cloudProvider.account.delete myAccountName


belongs to the namespace "cloudProvider" with a "sub-space" of "account" which 
gives context to the command "delete."

The purpose of this tree-based command structure is not only readability but 
also maintainability of the code.  This allows a "namespace" class to be defined
at the top level with sub-space child classes and context-aware command-specific
classes at the base.  The result is a simpler process of routing commands to the
appropriate underlying functionality.



IV. A.	Namespace: None (General Commands)


IV. A. 1.	COMMAND: quit | exit

	STATUS: In development
	
	Either typing quit or exit allows the user to exit the current shell
	(which implies a log out if the current shell is the main shell, which
	is an important note when nesting shells).


	
IV. A. 2.	COMMAND: help [command | topic | list]
	
	STATUS: In development
	
	Typing "help" will return the general help document unless a context is
	provided (e.g. a command or topic string.  The "context" parameter will 
	cause the relevant file to be displayed.
	
	If the "help list" option is invoked, a list of all help documents will be
	rendered to the display.

	

IV. B.	Namespace: Server (General Server Operations)



IV. B. 1.	COMMAND: server.configure [--interactive]
	
	STATUS: In development
	
	This command is used one time only during deployShell system implementation.
	It has an optional "interactive" mode which will walk the user through a 
	wizard-based setup program and output both an active, running server as well
	as a configuration file for this running server that can be downloaded and
	used to deploy clone deployShell servers.


	
IV. B. 2.	COMMAND: 

	STATUS: 



IV. C. 	Namespace: cloudProvider



IV. C. 1.	COMMAND: cloudProvider.account.add myAccountName amazonAWS

	STATUS: Not Started



IV. C. 1.	COMMAND: cloudProvider.account.delete myAccountName

	STATUS: Not Started



IV. C. 1.	COMMAND: cloudProvider.account.disable myAccountName

	STATUS: Not Started



IV. C. 1.	COMMAND: cloudProvider.account.enable myAccountName

	STATUS: Not Started



IV. C. 1.	COMMAND: cloudProvider.account.enforcePolicy myPolicyName

	STATUS: Not Started



IV. C. 1.	COMMAND: cloudProvider.account.policyApply acctName polName <enforce|passive>

	STATUS: Not Started



IV. C. 1.	COMMAND: cloudProvider.account.policyAudit myPolicyName

	STATUS: Not Started



IV. C. 1.	COMMAND: cloudProvider.account.relaxPolicy myPolicyName

	STATUS: Not Started



IV. C. 1.	COMMAND: cloudProvider.account.removePolicy myAccountName myPolicyName

	STATUS: Not Started




IV. D.	Namespace: group

IV. D. 1. COMMAND: group.addUser groupName myUsername

IV. D. 2. COMMAND: group.addUsers groupName "<comma-delimited list of usernames>"

IV. D. 3. COMMAND: group.addPolicy myGroupName myPolicyName [enforce|passive]

IV. D. 4. COMMAND: group.auditPolicy myGroupName


IV. D. 5. COMMAND: group.create groupName

IV. D. 6. COMMAND: group.delete myGroupName


IV. D. 7. COMMAND: group.list

IV. D. 8. COMMAND: group.members groupName

IV. D. 9. COMMAND: group.removeUser myGroupName myUserName

IV. D. 10. COMMAND: group.removePolicy myGroupName myPolicyName



IV. E. 	Namespace: user


IV. E. 1. COMMAND: user.create <role> uName --email me@dn.tld --key"<sshPubKey>"

IV. E. 2. COMMAND: user.delete userName [--purge]
	




=============================================================================
V.	 Mapping: Abstracting Cloud Resources to Integrate Cloud Providers
=============================================================================

To make the deployShell solution vendor agnostic, the system must devise a 
means of mapping vendor-specific cloud lexicons into an abstract lexicon.  
This allows recipes to define server configurations without consideration of
the cloud hosting provider to be supported.  For example, an engineer can 
define a recipe, identifying the number of CPU cores, amount of RAM, disk
expectations and cost parameters.  This allows the same recipe to define a
server environment on any hosting provider without change.  In order for this
to work, the system must have an internal lexicon for use within the recipes
and a system of mapping between vendor-specific lexicons and this internal
lexicon.

The internal mapping files which define the deployShell resource lexicon are
found in conf/mapping/, written in JSON.  There are three (3) types of maps:

	(1) image,
	
	(2) location,
	
	(3) size

Mapping in general consists of three (3) problems:

	(1) Generic Map Creation
	
	(2) Vendor-Feed Download and Parsing.

	(3) Vendor-Feed to Generic Map Process
	
The first problem is addressed during the software development process when
the local maps found in conf/mapping are created.  These maps define the 
resource terms understood by deployShell and made available to developers via 
recipes.

The second problem is addressed in each vendor-specific API interface, which
will connect to the vendor's API and download the appropriate resource list
(in the vendor-specific format).  It then parses and transforms the API
return (often a JSON or XML output) into a standardized data structure.

The standardized data structure is then "mapped" to the relevant local map to
create a vendor- and domain-specific map file (also in conf/mapping) which 
bears the name of the cloud vendor and account as well as the type of map,
e.g. 


    +-----------------------------------------+-----------------------------+
    |Map File Path                            |Description                  | 
    +-----------------------------------------+-----------------------------+
    |conf/mapping/images                      |provides the local resource  |
    |                                         |map for images.              |
    +-----------------------------------------+-----------------------------+
    |conf/mapping/...map|provides the vendor- and     |
    |                                         |domain-specific map.         | 
    +-----------------------------------------+-----------------------------+
    |Note: When we refer to a "vendor- and domain-specific map," we refer to|
    |      a map that pertains to the cloud hosting provider and its local  |
    |      account configuration file as well as to the map type (domain).  |
    +-----------------------------------------------------------------------+


Because vendor-specific API interface code must access the local resource maps
during the mapping process, a common Python library (mapHandlers.py) exists
and this library provides the "MapClass" class for loading and writing map 
files to conf/mapping.

Map names should observe the following conventions:

	(1) Map names must be less than forty (40) characters in length.
	(2) Map names must contain only alpha-numeric strings (including underscore
	    and hyphen) but no spaces or other characters.


IV. A. Standardized Mapping API Interface Structures

Each API Interface is substantially different internally as it must confront a
different API interaction problem as well as a different data-transformation
problem.  However, the API Interfaces have some commonalities across the 
spectrum of cloud providers, to wit:

    (1) Each API interface must consist of the following three (3) wrapper 
        scripts: apiProcessImageFeed, apiProcessLocationFeed and
        apiProcessSizeFeed.  These are the feed processing API calls the
        deployShell system use to pull down and map new data.

	(2) Each API interface must have an install script which will setup the
	    dependencies for the feed processors as well as other interface code.
	    
	(3) Each API interface will maintain the specialized feed processing code
        in the api//bin/ directory.


IV. B. Integrating API Interface Structures into the deployShell System

Most of the functionality of deployShell is driven by user interaction with 
the system's main shell script (~/deployShell/deployShell).  However feed
processing for resource maps is not driven by the user interaction.  It is 
instead driven by the deploy user's crontab.  This crontab file can be
managed by user interaction--

	See enableFeedJobSchedule, disableFeedJobSchedule, listJobs in the
	command reference or the online help file.
	
When the cron job fires, it executes "cron/jobMaster" as the wrapper script
to setup the runtime environment before passing control to the appropriate
top-level feed processor (e.g. processImageFeed, processLocationFeed, 
processSizeFeed).  These top-level feed processor scripts (written in Bash)
will setup and route the work to the appropriate vendor API interface with
the correct vendor-specific parameters (e.g. tokens).


IV. C. Local Resource Map Structures

Local resource map files are found in conf/mapping/ and are written in JSON
as a single, domain-specific object.  Each file consists of a header and a
map array containing the location objects.


IV. C. 1. Location Map JSON Structure

Location maps create geographic regions to which deployShell can deploy 
infrastructure.  These geographic regions are described by--

	+----+----------------------------------------------------------------+
	|Loc | A "friendly name" the recipe set can use to define the data    |
	|    | center.                                                        |
	+----+----------------------------------------------------------------+
	|desc| A "description" of the location for human-readable purpose.    |
	+----+----------------------------------------------------------------+
	|long| The longitude of the region's center of mass.                  |
	+----+----------------------------------------------------------------+
	|lat | The latitude of the region's center of mass.                   |
	+----+----------------------------------------------------------------+
	
The original location map is shown below:

{
	"class":"map",
	"type":"images",
	"scope":"local",
	"updated":1405393850,
	"map":[ 
		{"loc":"usnw","desc":"US North West","long":0,"lat":0},
		{"loc":"usne","desc":"US North East","long":0,"lat":0},
		{"loc":"usnc","desc":"US North Central","long":0,"lat":0},
		{"loc":"ussc","desc":"US South Central","long":0,"lat":0},
		{"loc":"usse","desc":"US South East","long":0,"lat":0},
		{"loc":"ussw","desc":"US South West","long":0,"lat":0}
	]
}

(NOTE: The longitude and latitude in the above descriptions have a zero-value.
       They should be populated before production.)



IV. C. 2. Size Map JSON Structure

So-called "Size" maps describe resource constraints for servers created with the
deployShell system.  Factors considered here include CPU speed, core count, RAM
size, disk configuration, IOPS, bandwidth and networking as well as operating 
cost per hour.

The local size map, unlike location and size, does not focus on mapping a
vendor-specific lexicon to an abstract lexicon.  With location, for example, a
data center in New York maps easily to the US North West region (usnw).  But in
the size abstraction, the local resource map conf/mappings/size simply defines
what size-resource objects may be considered in a recipe.  


{
    "class":"map",
    "type":"size",
    "scope":"local",
    "updated":1405393850,
    "map":[ 
        {
            "factor":"cpuCores",
            "desc":"Number of CPU Cores",
            "type":"integer"
        },
        {
			"factor":"RAM",
			"desc":"Amount of Memory",
			"type":"integer"
		},
		{
			"factor":"Bandwidth",
			"desc":"Total Allowed Bandwidth (KB) included",
			"type":"integer"
		},
		{
			"factor":"DiskSize",
			"desc":"Disk Size included in boot image.",
			"type":"integer"
		},
		{
			"factor":"canAddDisks",
			"desc":"Disk Size included in boot image.",
			"type":"Boolean"
		},
		{
			"factor":"hasPrivateNetworking",
			"desc":"Supports private network interface.",
			"type":"Boolean"
		},
		{
			"factor":"hasNAT",
			"desc":"Network uses NAT",
			"type":"Boolean"
		},
		{
			"factor":"hourlyBaseCost",
			"desc"	:"Base cost for the server node.",
			"type"	:"float"
		},
		{
			"factor":"addedDiskCost",
			"desc":"Cost of added disks per MB/hr",
			"type":"float"
		},
		{
			"factor":"canAddStaticIP",
			"desc":"Can add static IP addresses to servers.",
			"type":"Boolean",
		}
	]
}



IV. C. 3. Image Map JSON Structure

	
IV. B. Mapping Location

Mapping location of data centers where the server instances will run is the
easiest of the three mapping types.  Yet it is neither an exact science nor
is it a simple solution.  The problem is defining how you might map the data
center locations of vendors A, B and C to some generic dictionary of locations
and still have a useful location for the users to geographically disburse 
services for disaster prevention/recovery.

deployShell maps locations based on proximity to an arbitrary central point of
geographic regions.  Currently the mapping strategy divides the United States
into six (6) regions:


	+------------------------+
	|REGION	DESCRIPTION      |
	+------------------------+
	|usnw:   US-NorthWest    |
	|usne:   US-NorthEast    |
	|usnc:   US-NorthCentral |
	|ussc:   US-SouthCentral |
	|usse:   US-SouthEast    |
	|ussw:   US-SouthWest    |
	+------------------------+

Determining the proper region for a given datacenter is a matter of geolocating
the data center and determining the region whose center-mass has the closest 
proximity.  This requires that each region have a center identified by longitude 
and latitude against which data center locations can be compared.  The challenge
arrives if some data center is located at the mid-point between the center of
two regions.  However, this is so unlikely that it is not addressed in the 
current project.  But it will probably arise at some point in the future and 
should thus be noted.

Automation for regionalization is planned for an unknown later date.  Currently,
data center regions are mapped manually by an arbitrary process since the data
centers supported are sufficiently distant from one another.


IV. B. Mapping Images

Mapping operating system images in deployShell is a little more than simply
measuring the distance between two points and dropping the result in a bucket.
Operating systems are classified with three (3) factors:

	(1) Distribution (e.g. CentOS,Debian,Ubuntu)
	
	(2) Version (e.g. 12.04, 14.04, etc.)
	
	(3) Architecture (x32, x64)

To avoid complexity and a maintenance nightmare, only the images which are
currently supported by ALL of the enabled vendors should ever appear as mapped
images.  This will require periodic "pruning" of image maps in some later
version.  As with other mapping problems, this is currently a manual process.


IV. C. Mapping Sizes

Server instance sizes determine cost as well as technology resources.  The 
variations are wide ranging and ever-changing.  A static mapping of server size
across the cloud ecosystem is not likely to succeed for this reason.  Therefore,
"size mapping" is extended into a different direction.  People do not choose a
vendor's sizing because it meets their specific needs.  They choose the closest
approximation to their needs based on specific criteria that do matter.

The deployShell server-size mapping likewise performs an approximation through 
the recipe, where its author defines the criteria that do matter, leaving the
rest to their system default.  For example, a recipe defines--

	(1) Number of CPU Cores
	(2) Percentage of CPU Share
	(3) Minimum RAM
	(4) Maximum RAM
	(5) Minimum Disk Size
	(6) Maximum Disk Size
	(7) Minimum IOPS
	(8) Maximum IOPS
	(9) Maximum Hourly Cost

The shell then evaluates the desired cloud hosting vendor, region and image to 
determine which server_size is both available and closely approximate to the
requirements.  Any unsupported requirement is assumed to be met and any infeasible
minimum or maximum is likewise defaulted to an acceptance.  In the end, the 
author gets a server configuration that most closely meets his/her needs.

The problem comes when two requirements have the same approximation for two or 
more solutions.  In these cases, factors must be assigned a weight to determine
what matters more in the case of a tie.  This means that the recipe must define
the following:


	-----------------------------------------------------
	Mapping Criteria            Value       Weight
	-----------------------------------------------------
	Number of CPUs              nCPU        nCPUw
	Percentage of CPU Share     pCPU        pCPUw
	Minimum RAM                 mnRAM       mnRAMw
	Maximum RAM                 mxRAM       mxRAMw
	Minimum Disk Size           mnDS        mnDSw
	Maximum Disk Size           mxDS        mxDSw
	Minimum IOPS                mnIO        mnIOw
	Maximum IOPS                mxIO        mxIOw
	Maximum Hourly Cost         mxCost      mxCostw
	-----------------------------------------------------


IV. D. Reading and Writing Map Files


=============================================================================
V. Mapping: Dynamic Feeds and Mapping
=============================================================================

In order to create maps between abstract deployShell terms and those of the
many cloud hosting providers and their API, software must exist which will
update the maps dynamically.  These software "mapping agents" execute through
the deployShell server's cron daemon to poll the enabled cloud hosting provider
API for entities which can be applied to local maps.

In the case of location and image maps, the idea of downloading feeds from the
cloud providers then building the mapping files found in conf/mappings is 
straight forward.  However, extending this to server size, the matter becomes
less clear.  So-called "map information" downloaded for server-size mappings 
does not create a static map from deployShell to the cloud provider.  Instead
it reduces the network load of requesting information each time a recipe is
processed.  It also reduces the deployment latency (i.e. the time required for
a recipe to be processed from text file into a running server instance.

V. A. Dynamic Feed Job: SizeFeedProcessor

The first "dynamic feed job" is the "sizeFeedProcessor" which downloads feeds 
from all of the enabled cloud hosting providers.  This job executes the 
following process:

    (1) Determine the list of enabled cloud provider API.
  
    (2) Iterate through the list for each element:
        (2)(a) Connect to the API and fetch a list of possible server sizes.
        (2)(b) Parse the list for each of the mapping criteria and write the
               results to a size-map file (conf/mappings/size).
                   
The job executes as per the "Dynamic Feed Job Schedule."


V. B. Dynamic Feed Job: LocationFeedProcessor

Locations are mapped by the LocationFeedProcessor, which downloads a list of all
data center locations maintained by a cloud hosting provider then maps the same
by geolocation into deployShell geographic regions (see above).  This requires a
more complex process than V. A.:

    (1) Test whether conf/mappings/location.raw exists.
        (1)(a) If file exists--
            (1)(a)(1) Log that job will not run (blocked_in_progress).
            (1)(a)(2) Terminate successfully.
        (1)(b) Else--
            (1)(b)(1) Log that job will run (not_blocked).
            
    (2) Determine the list of enabled cloud provider API.
    
    (3) Iterate through the list for each element:
        (3)(a) Connect to the API and download a list of possible data centers.
        (3)(b) Parse the list to create a temporary map file
               (conf/mappings/location.raw).
    
    (4) Iterate through the temporary map file for each data center:
        (4)(a) Spawn a mapping thread (locationFeedMapper) to geolocate the 
        	   data centers and return the appropriate deployShell region 
               string.
   
    (5) Update the temporary map file with the data center locations and when 
    	all are updated, copy conf/mappings/location.raw to conf/mappings/
    	locations and delete the raw file.


V. C. Dynamic Feed Job: ImageFeedProcessor

Image feeds are mapped by the ImageFeedProcessor which downloads the list of 
all possible images provided by a hosting provider before mapping them to the 
generic deployShell nomenclature.  Of the mapping processes, this one is the 
most simple to document:

    (1) Determine the list of enabled cloud provider API.
    (2) Iterate through the list for each element.
        (2)(a) Connect to the API and download the list of possible images.
        (2)(b) Parse the list to create a map file.


V. D. Dynamic Feed Job Schedule

The feed job schedule executes under the deploy user's crontab, limiting the 
job's potential permissions to that of deploy for security reasons.  Each of 
these jobs are built-in and perform only the tasks described above.  But from 
time to time a systems administrator may want to enable/disable the feed jobs 
(for example, to avoid a deployShell outage when the feeds fail to update 
during a network outage, a systems administrator may decide to disable the 
schedule, among other reasons.  To do this, the sysadmin simply executes:

        disableFeedJobSchedule
        
When any outage/maintenance is complete, the sysadmin executes: 

        enableFeedJobSchedule

and the feed job schedule will resume.

=============================================================================
VI.     Recipes: A Beginning
=============================================================================

VI. A. Sections of a Recipe

	(1) Server Requirements
		(a) Cost
		(b) Location
		(c) Op/Sys
		(d) RAM, CPU, Disk, I/O
		(e) Cloud hosting provider.
	(2) Server Configuration
		(a) Partition Scheme
		(b) Network Configuration
		(c) Initial Package Load
		(d) Firewall Configuration
	(3) Role Configuration
		(a) Pre-deploy Script
		(b) Tag Processing
		(c) Post-deployScript
		(d) QA Validation

=============================================================================
VII.    Recipes: Adding Tags, Pre-Deploy and Post-Deploy Scripts
=============================================================================

This section extends Section VI to further explore the role of pre-deploy 
scripts, Tag processing, and post-deploy scripts within the deployShell
ecosystem.  These concepts extend deployShell both in their integration with
and across cloud hosting providers as well as allowing for the deployment of
rich environments.


VII. A. Order of Operations

As the names of each of the constructs covered in this section suggests, the
order of operations is pre-deployment, tags (processed left to right), then
post-deployment.  The content and operation of the pre-deployment and post-
deployment scripts is arbitrary.  These scripts reside in the public recipe
repository and are executed remotely on the target server before and after the
tag list is processed.  When considering the order in which pre- and post-
deployment scripts are executed, one should properly consider the role of the
pre-deployment script as an optional preparatory step prior to applying the 
tag list.  Most of the work not performed by the tag list should be performed
by the post-deployment script.

pre- and post-deployment scripts are executed on the target server by the
following construct:

	curl -s http://reciperepository.tld/deploy/ | bash
	
as the target server's root user via SSH.



=============================================================================
VIII.   Recipes: Quality Assurance through Unit Tests
=============================================================================

=============================================================================
IX.     Recipes: Advanced Recipes for Building Clusters
=============================================================================

=============================================================================
X.      User Management in deployShell
=============================================================================

X. A.   Users in deployShell

Users created in deployShell are Linux users with deployShell as their user
shell and /home/deploy as their home directory.  They authenticate with the
server using SSH keys but have no access to the underlying operating system.
Their only privilege is to execute deployShell commands and to use its feature
set.  These users will appear in /etc/passwd and /etc/shadow as Linux users,
but their privileges and other deployShell specific characteristics are found
in CONFDIR/users.

Users have two types of permissions: general and promoted.  Users with general
permissions have the ability to deploy servers, delete servers, list objects
and in general execute the deployment objective of the deployShell.  However,
the general user privilege does not allow users to manage user or cloud 
provider accounts.  Likewise general privileges do not allow the enabling or
disabling of a cloud provider's API.  To do these non-general (or "promoted"
tasks), the user must have his/her account "promoted," becoming a super user.


X. B.   Creating deployShell Users



X. C.   Deleting deployShell Users



X. D.   Promoting deployShell Users



X. E.   Demoting deployShell Users



X. F.   Listing deployShell Users







				
